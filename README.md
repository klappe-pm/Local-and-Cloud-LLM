# Local and Cloud LLM Development Environment

A comprehensive AI development environment integrating local and cloud-based Large Language Models (LLMs) with MCP server orchestration.

## ğŸš€ Overview

This repository contains configuration and documentation for managing multiple AI model providers through a unified interface, including:

- **Cloud Providers**: Anthropic Claude, Google Gemini, OpenAI GPT-4, X.AI Grok
- **Local Models**: Ollama-managed models (Qwen, CodeLlama, Llama)
- **MCP Integration**: Claude Desktop with enhanced MCP servers
- **API Management**: Secure, project-specific API key configuration

## ğŸ“‹ Features

- âœ… Multi-provider AI model access
- âœ… Secure API key management
- âœ… MCP server integration for Claude Desktop
- âœ… Local model support via Ollama
- âœ… Comprehensive documentation and setup guides
- âœ… Git-ready with security best practices

## ğŸ—‚ Project Structure

```
.
â”œâ”€â”€ API/                    # API documentation and setup instructions
â”‚   â”œâ”€â”€ API_SETUP_INSTRUCTIONS.md
â”‚   â”œâ”€â”€ Models.md          # Available models reference
â”‚   â””â”€â”€ README.md          # API folder overview
â”œâ”€â”€ Requirements/          # Project requirements and guidelines
â”‚   â””â”€â”€ GITIGNORE_REQUIREMENTS.md
â”œâ”€â”€ Keys/                  # Sensitive files (gitignored)
â”œâ”€â”€ .env                   # Environment variables (gitignored)
â”œâ”€â”€ load_env.sh           # Environment loader script (gitignored)
â””â”€â”€ README.md             # This file
```

## ğŸ›  Quick Setup

### 1. Clone the Repository

```bash
git clone https://github.com/klappe-pm/Local-and-Cloud-LLM.git
cd "Local and Cloud LLM"
```

### 2. Set Up API Keys

```bash
# Create .env file with your API keys
cp .env.example .env
# Edit .env with your actual keys

# Load environment
source ./load_env.sh
```

### 3. Configure Claude Desktop

Update MCP servers in Claude Desktop configuration:
- Location: `~/Library/Application Support/Claude/claude_desktop_config.json`
- See `API/API_SETUP_INSTRUCTIONS.md` for detailed configuration

### 4. Install Local Models (Optional)

```bash
# Install Ollama
brew install ollama

# Pull models
ollama pull llama3.2
ollama pull codellama:34b
ollama pull qwen2.5-coder:32b  # Large model - 19GB
```

## ğŸ”‘ Available Providers

| Provider | Models | Status | Access Method |
|----------|--------|--------|---------------|
| Google Gemini | Flash, Pro | âœ… Active | MCP Zen Server |
| OpenAI | GPT-4.1, O3, O4 | âœ… Active | MCP Zen Server |
| X.AI | Grok-3 | âœ… Active | MCP Zen Server |
| Anthropic | Claude Opus 4.1 | âœ… Active | Claude Desktop |
| Local/Ollama | Llama, CodeLlama, Qwen | âœ… Active | localhost:11434 |
| OpenRouter | Multiple | ğŸ”´ Configured | Not detected by MCP |

## ğŸ“š Documentation

- **[API Setup Instructions](API/API_SETUP_INSTRUCTIONS.md)**: Complete setup guide
- **[Available Models](API/Models.md)**: Model reference and selection guide
- **[Git Ignore Requirements](Requirements/GITIGNORE_REQUIREMENTS.md)**: Security best practices

## ğŸ”’ Security

This project follows security best practices:

- API keys are stored in `.env` files (never committed)
- Comprehensive `.gitignore` prevents credential leaks
- Project-specific configuration (not global)
- Regular key rotation recommended

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

**Important**: Never commit API keys or sensitive information!

## ğŸ“ License

[Your License Here]

## ğŸ‘¤ Author

Kevin Lappe

## ğŸ”— Links

- [Claude Desktop](https://claude.ai)
- [Ollama](https://ollama.ai)
- [MCP Protocol](https://modelcontextprotocol.io)

---

*Last Updated: December 2024*
